{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50853452-41f7-4457-96fb-3c54e4b31801",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Gradient Boosting Regression?\n",
    "\n",
    "\n",
    "ANS-1\n",
    "\n",
    "\n",
    "Gradient Boosting Regression is a machine learning technique that belongs to the family of ensemble methods and is used for regression tasks. It is an extension of the Gradient Boosting Machine (GBM) algorithm, which is primarily used for classification problems.\n",
    "\n",
    "In Gradient Boosting Regression, the goal is to build a predictive model that can accurately predict continuous numerical values (i.e., the target variable) based on a set of input features. The algorithm works by sequentially training weak learners (usually decision trees) and then combining their predictions to form a strong ensemble model.\n",
    "\n",
    "The key idea behind Gradient Boosting Regression is to fit each weak learner to the negative gradient of the loss function with respect to the current ensemble's predictions. In other words, each weak learner focuses on the errors made by the previous ones and tries to correct them.\n",
    "\n",
    "Here's how the Gradient Boosting Regression works:\n",
    "\n",
    "1. **Initialization**: The ensemble starts with an initial prediction, which is usually set to the mean value of the target variable in the training data.\n",
    "\n",
    "2. **Compute Residuals**: The residuals (or errors) are computed by subtracting the current ensemble's predictions from the true target values. These residuals represent the errors that the next weak learner needs to correct.\n",
    "\n",
    "3. **Train Weak Learner**: A weak learner (e.g., decision tree) is trained on the data, but instead of using the original target values, it uses the computed residuals as the new target values. The weak learner's goal is to find patterns in the data that can help reduce the remaining errors in the current ensemble.\n",
    "\n",
    "4. **Compute Weights (Learning Rate)**: The predictions of the weak learner are scaled by a factor known as the learning rate. The learning rate controls the contribution of each weak learner to the ensemble and helps prevent overfitting. A smaller learning rate means each weak learner has a smaller impact on the final prediction.\n",
    "\n",
    "5. **Update Ensemble**: The predictions of the current weak learner, scaled by the learning rate, are added to the current ensemble's predictions. This update reduces the errors and improves the ensemble's performance.\n",
    "\n",
    "6. **Repeat Steps 2 to 5**: Steps 2 to 5 are repeated for a predefined number of iterations (or until a stopping criterion is met). In each iteration, a new weak learner is trained on the updated residuals, and the ensemble is iteratively improved.\n",
    "\n",
    "7. **Final Prediction**: The final prediction of the Gradient Boosting Regression model is the sum of the predictions from all weak learners in the ensemble.\n",
    "\n",
    "Gradient Boosting Regression is a powerful technique for solving regression problems, and it is known for its ability to handle complex, nonlinear relationships in the data. However, like other boosting algorithms, it requires careful hyperparameter tuning and may be computationally expensive for large datasets or deep trees.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared.\n",
    "\n",
    "\n",
    "\n",
    "ANS-2\n",
    "\n",
    "\n",
    "\n",
    "Implementing a complete Gradient Boosting algorithm from scratch can be quite involved, but I can provide you with a simplified version using Python and NumPy. We'll use a simple dataset with one input feature and one target variable for regression. Please note that this implementation is for educational purposes and may not be as optimized as popular libraries like scikit-learn or XGBoost.\n",
    "\n",
    "Let's proceed with the implementation:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        self.residuals = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.trees = []\n",
    "        self.residuals = np.copy(y)\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, self.residuals)\n",
    "            prediction = tree.predict(X)\n",
    "            self.trees.append(tree)\n",
    "            self.residuals -= self.learning_rate * prediction\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros(X.shape[0])\n",
    "        for tree in self.trees:\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "        return predictions\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def r_squared(y_true, y_pred):\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    return 1 - (ss_res / ss_tot)\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate a simple dataset for regression\n",
    "    np.random.seed(42)\n",
    "    X = np.random.rand(100, 1)\n",
    "    y = 2 * X[:, 0] + np.random.normal(0, 0.1, 100)\n",
    "\n",
    "    # Split the dataset into training and test sets\n",
    "    X_train, X_test = X[:80], X[80:]\n",
    "    y_train, y_test = y[:80], y[80:]\n",
    "\n",
    "    # Train the Gradient Boosting Regressor\n",
    "    gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "    gbr.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = gbr.predict(X_test)\n",
    "\n",
    "    # Evaluate the model's performance\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r_squared(y_test, y_pred)\n",
    "\n",
    "    print(\"Mean Squared Error:\", mse)\n",
    "    print(\"R-squared:\", r2)\n",
    "```\n",
    "\n",
    "In this implementation, we define the `GradientBoostingRegressor` class, which fits the ensemble of decision trees using the gradient boosting approach. We also implement the mean squared error and R-squared functions to evaluate the model's performance.\n",
    "\n",
    "Please note that this is a simplified version, and a complete implementation would require additional considerations, such as handling categorical features, early stopping to prevent overfitting, and implementing additional hyperparameters. For production use, it is recommended to use well-established libraries like scikit-learn or XGBoost, which provide more extensive functionalities and optimizations.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters\n",
    "\n",
    "\n",
    "ANS-3\n",
    "\n",
    "\n",
    "To experiment with different hyperparameters and optimize the performance of the Gradient Boosting Regressor, we can use Grid Search or Random Search to find the best combination of hyperparameters. Grid Search exhaustively tries all possible combinations of specified hyperparameter values, while Random Search randomly samples hyperparameter combinations within the specified ranges.\n",
    "\n",
    "Below, I'll demonstrate how to use Random Search to find the best hyperparameters for the Gradient Boosting Regressor on the example dataset:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    # ... (same implementation as before) ...\n",
    "\n",
    "# Generate a simple dataset for regression\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1)\n",
    "y = 2 * X[:, 0] + np.random.normal(0, 0.1, 100)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a GradientBoostingRegressor instance\n",
    "gbr = GradientBoostingRegressor()\n",
    "\n",
    "# Define the hyperparameter grid for random search\n",
    "param_grid = {\n",
    "    'n_estimators': np.arange(50, 201, 10),\n",
    "    'learning_rate': np.logspace(-3, 0, 20),\n",
    "    'max_depth': np.arange(2, 11),\n",
    "}\n",
    "\n",
    "# Perform Random Search with 5-fold cross-validation\n",
    "random_search = RandomizedSearchCV(gbr, param_distributions=param_grid, n_iter=50, cv=5, random_state=42, n_jobs=-1)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and the corresponding model\n",
    "best_params = random_search.best_params_\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance with the best hyperparameters\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n",
    "```\n",
    "\n",
    "In this example, we use `RandomizedSearchCV` from scikit-learn to perform Random Search with 5-fold cross-validation to find the best hyperparameters. The `param_grid` defines the ranges of hyperparameter values to explore. We specify the number of iterations (n_iter) for Random Search (50 in this case). The best hyperparameters and the corresponding model are obtained from the results of Random Search, and we evaluate the model's performance on the test set using these optimal hyperparameters.\n",
    "\n",
    "You can adjust the hyperparameter grid and the number of iterations as needed to further fine-tune the model's performance. Keep in mind that Random Search can be a computationally efficient alternative to Grid Search when dealing with a large hyperparameter search space.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "\n",
    "\n",
    "\n",
    "ANS-4\n",
    "\n",
    "\n",
    "\n",
    "In Gradient Boosting, a weak learner refers to a simple and relatively low-complexity model that performs slightly better than random guessing on the training data for a specific learning task. The term \"weak\" does not imply that the model is ineffective; rather, it means that the model has limited predictive power compared to more complex models.\n",
    "\n",
    "In the context of Gradient Boosting, weak learners are typically decision trees with restricted depth (also known as decision stumps) or shallow trees with only a few levels. Decision stumps are often used as weak learners because they are the simplest form of decision trees and consist of a single split. They make decisions based on a single feature and can only create binary splits.\n",
    "\n",
    "The key idea behind Gradient Boosting is to combine multiple weak learners in an ensemble to create a strong learner. Each weak learner focuses on the errors made by the previous learners and attempts to correct them. In other words, weak learners try to improve the shortcomings of the model sequentially, and the ensemble effectively benefits from the collective wisdom of these weak learners.\n",
    "\n",
    "By sequentially adding weak learners to the ensemble, Gradient Boosting becomes a powerful and flexible technique that can handle complex, nonlinear relationships in the data. The adaptive nature of Gradient Boosting allows it to focus on the data points that are harder to predict, leading to improved performance on the training and test datasets.\n",
    "\n",
    "It's important to note that while weak learners are individually simple, the ensemble of multiple weak learners can form a strong, high-performing model that exhibits excellent generalization capabilities. The success of Gradient Boosting lies in the ability to iteratively learn from the errors made by the weak learners and combine their predictions to make accurate and robust predictions on the data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "\n",
    "\n",
    "ANS-5\n",
    "\n",
    "\n",
    "\n",
    "The intuition behind the Gradient Boosting algorithm lies in the idea of creating a strong predictive model by sequentially improving upon the mistakes of weak learners. The algorithm combines multiple weak learners (usually decision trees) into an ensemble to create a powerful model that can make accurate predictions.\n",
    "\n",
    "Here's the intuition behind the Gradient Boosting algorithm:\n",
    "\n",
    "1. **Starting Point**: The ensemble starts with a simple model, typically initialized with the mean (for regression) or the majority class (for classification) of the target variable. This initial model is a weak learner, as it is not very accurate but provides a starting point.\n",
    "\n",
    "2. **Sequential Improvement**: In each iteration, a new weak learner is trained to correct the errors made by the current ensemble of learners. The new learner focuses on the data points that were misclassified or poorly predicted by the current ensemble.\n",
    "\n",
    "3. **Gradient Descent**: The name \"Gradient Boosting\" comes from the gradient descent optimization technique used during training. The weak learner is fitted to the negative gradient (or the \"residuals\") of the loss function with respect to the current ensemble's predictions. This allows the new learner to adjust the model's predictions in the direction that reduces the errors in the ensemble.\n",
    "\n",
    "4. **Ensemble Combination**: After training a new weak learner, its predictions are combined with the predictions of the previous ensemble. The combination is weighted, with each weak learner's contribution determined by a learning rate, which controls the step size taken towards the optimal solution.\n",
    "\n",
    "5. **Iterative Learning**: The process is repeated for a specified number of iterations (or until a stopping criterion is met). In each iteration, a new weak learner is trained and added to the ensemble, iteratively improving the model's predictions.\n",
    "\n",
    "6. **Model Adaptation**: Gradient Boosting adapts to the data during training. By focusing on the errors made by previous learners, it pays more attention to the samples that are harder to predict correctly. This adaptive nature enables the model to handle complex, nonlinear relationships in the data.\n",
    "\n",
    "7. **Combining Weak Learners**: The final prediction is made by combining the predictions of all weak learners. Each weak learner's prediction is weighted based on its contribution to the ensemble, with higher weight given to more accurate learners.\n",
    "\n",
    "The intuition behind Gradient Boosting is to build a strong model by learning from the mistakes of weak learners. Each weak learner is specialized in capturing certain patterns in the data, and their collective knowledge is used to make accurate predictions. By iteratively adding weak learners and adjusting their contributions, the ensemble gradually improves and achieves high accuracy on the training data as well as strong generalization to new, unseen data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ANS-6\n",
    "\n",
    "\n",
    "\n",
    "The Gradient Boosting algorithm builds an ensemble of weak learners in a sequential and adaptive manner. The process involves iteratively training weak learners (usually decision trees) and combining their predictions to create a strong ensemble model. The key idea is to fit each weak learner to the negative gradient (or \"residuals\") of the loss function with respect to the current ensemble's predictions. This allows the new learner to focus on correcting the errors made by the previous learners.\n",
    "\n",
    "Here's a step-by-step explanation of how the Gradient Boosting algorithm builds an ensemble of weak learners:\n",
    "\n",
    "1. **Initialization**: The ensemble starts with a simple model, typically initialized with the mean (for regression) or the majority class (for classification) of the target variable. This initial model is a weak learner, as it is not very accurate but provides a starting point.\n",
    "\n",
    "2. **Compute Residuals**: The residuals (or errors) are computed by subtracting the current ensemble's predictions from the true target values. These residuals represent the errors that the next weak learner needs to correct.\n",
    "\n",
    "3. **Training Weak Learner**: A weak learner (e.g., decision tree) is trained on the data, but instead of using the original target values, it uses the computed residuals as the new target values. The weak learner's goal is to find patterns in the data that can help reduce the remaining errors in the current ensemble.\n",
    "\n",
    "4. **Compute Weights (Learning Rate)**: The predictions of the weak learner are scaled by a factor known as the learning rate. The learning rate controls the contribution of each weak learner to the ensemble and helps prevent overfitting. A smaller learning rate means each weak learner has a smaller impact on the final prediction.\n",
    "\n",
    "5. **Update Ensemble**: The predictions of the current weak learner, scaled by the learning rate, are added to the current ensemble's predictions. This update reduces the errors and improves the ensemble's performance.\n",
    "\n",
    "6. **Repeat Steps 2 to 5**: Steps 2 to 5 are repeated for a predefined number of iterations (or until a stopping criterion is met). In each iteration, a new weak learner is trained on the updated residuals, and the ensemble is iteratively improved.\n",
    "\n",
    "7. **Combining Weak Learners**: The final prediction of the Gradient Boosting algorithm is the sum of the predictions from all weak learners in the ensemble. Each weak learner's prediction is weighted based on its contribution to the ensemble, with higher weight given to more accurate learners.\n",
    "\n",
    "By sequentially adding weak learners and adjusting their predictions based on the negative gradient of the loss function, the Gradient Boosting algorithm effectively builds an ensemble of models that complement each other's strengths and weaknesses. The adaptive nature of Gradient Boosting allows it to focus on the data points that are harder to predict, leading to improved performance and robustness in handling complex, real-world data.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
